{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOu+QAb/bXmuREWIfrH110/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"_3TM8g5pQs7W"},"outputs":[],"source":["pip install -q transformers datasets evaluate segments-ai\n","apt-get install git-lfs\n","git lfs install\n","huggingface-cli login"]},{"cell_type":"code","source":["from segments.utils import get_semantic_bitmap\n","\n","def convert_segmentation_bitmap(example):\n","    return {\n","        \"label.segmentation_bitmap\":\n","            get_semantic_bitmap(\n","                example[\"label.segmentation_bitmap\"],\n","                example[\"label.annotations\"],\n","                id_increment=0,\n","            )\n","    }\n","\n","\n","semantic_dataset = hf_dataset.map(\n","    convert_segmentation_bitmap,\n",")"],"metadata":{"id":"MwUCohGFQ5CW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["semantic_dataset = semantic_dataset.rename_column('image', 'pixel_values')\n","semantic_dataset = semantic_dataset.rename_column('label.segmentation_bitmap', 'label')\n","semantic_dataset = semantic_dataset.remove_columns(['name', 'uuid', 'status', 'label.annotations'])"],"metadata":{"id":"5wdgnJ0UQ5Vj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hf_dataset_identifier = f\"{hf_username}/{dataset_name}\"\n","\n","semantic_dataset.push_to_hub(hf_dataset_identifier)\n","\n","hf_dataset_identifier = \"segments/sidewalk-semantic\""],"metadata":{"id":"2tCBoCfOQ5ka"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","ds = load_dataset(hf_dataset_identifier)"],"metadata":{"id":"BkYtBz-UQ5yr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ds = ds.shuffle(seed=1)\n","ds = ds[\"train\"].train_test_split(test_size=0.2)\n","train_ds = ds[\"train\"]\n","test_ds = ds[\"test\"]"],"metadata":{"id":"huu-NjHmQ6IC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","from huggingface_hub import hf_hub_download\n","\n","repo_id = f\"datasets/{hf_dataset_identifier}\"\n","filename = \"id2label.json\"\n","id2label = json.load(open(hf_hub_download(repo_id=hf_dataset_identifier, filename=filename, repo_type=\"dataset\"), \"r\"))\n","id2label = {int(k): v for k, v in id2label.items()}\n","label2id = {v: k for k, v in id2label.items()\n","\n","num_labels = len(id2label)"],"metadata":{"id":"Bveagj5XRLRR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchvision.transforms import ColorJitter\n","from transformers import SegformerFeatureExtractor\n","\n","feature_extractor = SegformerFeatureExtractor()\n","jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1) \n","\n","def train_transforms(example_batch):\n","    images = [jitter(x) for x in example_batch['pixel_values']]\n","    labels = [x for x in example_batch['label']]\n","    inputs = feature_extractor(images, labels)\n","    return inputs\n","\n","\n","def val_transforms(example_batch):\n","    images = [x for x in example_batch['pixel_values']]\n","    labels = [x for x in example_batch['label']]\n","    inputs = feature_extractor(images, labels)\n","    return inputs\n","\n","\n","# Set transforms\n","train_ds.set_transform(train_transforms)\n","test_ds.set_transform(val_transforms)"],"metadata":{"id":"_CLERwR1RLsY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import SegformerForSemanticSegmentation\n","\n","pretrained_model_name = \"nvidia/mit-b0\" \n","model = SegformerForSemanticSegmentation.from_pretrained(\n","    pretrained_model_name,\n","    id2label=id2label,\n","    label2id=label2id\n",")"],"metadata":{"id":"atg8uJe7RL5z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import TrainingArguments\n","\n","epochs = 50\n","lr = 0.00006\n","batch_size = 2\n","\n","hub_model_id = \"segformer-b0-finetuned-segments-sidewalk-2\"\n","\n","training_args = TrainingArguments(\n","    \"segformer-b0-finetuned-segments-sidewalk-outputs\",\n","    learning_rate=lr,\n","    num_train_epochs=epochs,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    save_total_limit=3,\n","    evaluation_strategy=\"steps\",\n","    save_strategy=\"steps\",\n","    save_steps=20,\n","    eval_steps=20,\n","    logging_steps=1,\n","    eval_accumulation_steps=5,\n","    load_best_model_at_end=True,\n","    push_to_hub=True,\n","    hub_model_id=hub_model_id,\n","    hub_strategy=\"end\",\n",")"],"metadata":{"id":"AIW8QXZlRMNx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","import evaluate\n","\n","metric = evaluate.load(\"mean_iou\")\n","\n","def compute_metrics(eval_pred):\n","  with torch.no_grad():\n","    logits, labels = eval_pred\n","    logits_tensor = torch.from_numpy(logits)\n","    # scale the logits to the size of the label\n","    logits_tensor = nn.functional.interpolate(\n","        logits_tensor,\n","        size=labels.shape[-2:],\n","        mode=\"bilinear\",\n","        align_corners=False,\n","    ).argmax(dim=1)\n","\n","    pred_labels = logits_tensor.detach().cpu().numpy()\n","    # currently using _compute instead of compute\n","    # see this issue for more info: https://github.com/huggingface/evaluate/pull/328#issuecomment-1286866576\n","    metrics = metric._compute(\n","            predictions=pred_labels,\n","            references=labels,\n","            num_labels=len(id2label),\n","            ignore_index=0,\n","            reduce_labels=feature_extractor.reduce_labels,\n","        )\n","    \n","    # add per category metrics as individual key-value pairs\n","    per_category_accuracy = metrics.pop(\"per_category_accuracy\").tolist()\n","    per_category_iou = metrics.pop(\"per_category_iou\").tolist()\n","\n","    metrics.update({f\"accuracy_{id2label[i]}\": v for i, v in enumerate(per_category_accuracy)})\n","    metrics.update({f\"iou_{id2label[i]}\": v for i, v in enumerate(per_category_iou)})\n","    \n","    return metrics"],"metadata":{"id":"s9_PCnuFRMcM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import Trainer\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_ds,\n","    eval_dataset=test_ds,\n","    compute_metrics=compute_metrics,\n",")"],"metadata":{"id":"h4f5bcFIRMqi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"8ruk1RYuRaST"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["kwargs = {\n","    \"tags\": [\"vision\", \"image-segmentation\"],\n","    \"finetuned_from\": pretrained_model_name,\n","    \"dataset\": hf_dataset_identifier,\n","}\n","\n","feature_extractor.push_to_hub(hub_model_id)\n","trainer.push_to_hub(**kwargs)"],"metadata":{"id":"K9ztUFbCRhXg"},"execution_count":null,"outputs":[]}]}